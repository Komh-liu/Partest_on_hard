{
  "tasks": [
    {
      "metadata": {
        "task_type": "array_sum",
        "hardware": {
          "cpu": {
            "cores": 8,
            "threads": 16,
            "frequency": "3.6 GHz",
            "available": "True"
          },
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <vector>\n#include <omp.h>\n\nusing Array = std::vector<int>;\n\nint array_sum(const Array& arr) {\n    int sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < arr.size(); ++i) {\n        sum += arr[i];\n    }\n    return sum;\n}",
        "framework": "OpenMP"
      }
    },
    {
      "metadata": {
        "task_type": "matrix_multiply",
        "hardware": {
          "cpu": {
            "cores": 8,
            "threads": 16,
            "frequency": "3.6 GHz",
            "available": "True"
          },
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <vector>\n#include <cuda_runtime.h>\n\nusing Matrix = std::vector<int>;\n\n__global__ void matrixMultiplyKernel(const int* A, int N, int M, int* result) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        int sum = 0;\n        for (int k = 0; k < M; k++) {\n            sum += A[row * M + k] * A[col * M + k];\n        }\n        result[row * N + col] = sum;\n    }\n}\n\nvoid matrix_multiply(const Matrix& A, int N, int M, Matrix& result) {\n    int *d_A, *d_result;\n    size_t sizeA = N * M * sizeof(int);\n    size_t sizeResult = N * N * sizeof(int);\n\n    cudaMalloc(&d_A, sizeA);\n    cudaMalloc(&d_result, sizeResult);\n    cudaMemcpy(d_A, A.data(), sizeA, cudaMemcpyHostToDevice);\n\n    dim3 blockSize(16, 16);\n    dim3 gridSize((N + blockSize.x - 1) / blockSize.x, (N + blockSize.y - 1) / blockSize.y);\n\n    matrixMultiplyKernel<<<gridSize, blockSize>>>(d_A, N, M, d_result);\n\n    cudaMemcpy(result.data(), d_result, sizeResult, cudaMemcpyDeviceToHost);\n    cudaFree(d_A);\n    cudaFree(d_result);\n}",
        "framework": "CUDA"
      }
    },
    {
      "metadata": {
        "task_type": "graph_bfs",
        "hardware": {
          "cpu": {
            "cores": 8,
            "threads": 16,
            "frequency": "3.6 GHz",
            "available": "True"
          },
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <vector>\n#include <queue>\n#include <omp.h>\n\nusing Graph = std::vector<std::vector<int>>;\n\nvoid bfs(const Graph& graph, int start) {\n    std::vector<bool> visited(graph.size(), false);\n    std::queue<int> q;\n    q.push(start);\n    visited[start] = true;\n\n    while (!q.empty()) {\n        int level_size = q.size();\n        \n        #pragma omp parallel for\n        for (int i = 0; i < level_size; ++i) {\n            int current;\n            #pragma omp critical\n            {\n                current = q.front();\n                q.pop();\n            }\n\n            for (int neighbor : graph[current]) {\n                if (!visited[neighbor]) {\n                    #pragma omp critical\n                    {\n                        if (!visited[neighbor]) {\n                            visited[neighbor] = true;\n                            q.push(neighbor);\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
        "framework": "OpenMP"
      }
    }
  ]
}