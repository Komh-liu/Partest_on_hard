{
  "tasks": [
    {
      "metadata": {
        "task_type": "array_sum",
        "hardware": {
          "cpu": {
            "cores": 8,
            "threads": 16,
            "frequency": "3.6 GHz",
            "available": "True"
          },
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <mpi.h>\n#include <vector>\n#include <numeric>\n\nint array_sum(const std::vector<int>& arr) {\n    int rank, size;\n    MPI_Init(NULL, NULL);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int n = arr.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = (rank == size - 1) ? n : start + local_n;\n\n    for (int i = start; i < end; ++i) {\n        local_sum += arr[i];\n    }\n\n    int global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    MPI_Finalize();\n\n    return rank == 0 ? global_sum : 0;\n}",
        "framework": "MPI"
      }
    },
    {
      "metadata": {
        "task_type": "matrix_multiply",
        "hardware": {
          "cpu": {
            "cores": 8,
            "threads": 16,
            "frequency": "3.6 GHz",
            "available": "True"
          },
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <mpi.h>\n#include <vector>\n\nvoid matrix_multiply(const std::vector<std::vector<int>>& A, std::vector<std::vector<int>>& result) {\n    int rank, size;\n    MPI_Init(NULL, NULL);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = A.size();\n    if (rank == 0) {\n        result.resize(n, std::vector<int>(n, 0));\n    }\n\n    int rows_per_process = n / size;\n    int extra_rows = n % size;\n    int start_row = rank * rows_per_process + std::min(rank, extra_rows);\n    int end_row = start_row + rows_per_process + (rank < extra_rows ? 1 : 0);\n\n    std::vector<std::vector<int>> local_result(end_row - start_row, std::vector<int>(n, 0));\n\n    for (int i = start_row; i < end_row; ++i) {\n        for (int j = 0; j < n; ++j) {\n            for (int k = 0; k < n; ++k) {\n                local_result[i - start_row][j] += A[i][k] * A[k][j];\n            }\n        }\n    }\n\n    if (rank == 0) {\n        MPI_Gather(local_result.data(), local_result.size() * n, MPI_INT, result.data(), rows_per_process * n, MPI_INT, 0, MPI_COMM_WORLD);\n        for (int r = 1; r < size; ++r) {\n            int r_start_row = r * rows_per_process + std::min(r, extra_rows);\n            int r_end_row = r_start_row + rows_per_process + (r < extra_rows ? 1 : 0);\n            MPI_Gather(local_result.data(), (r_end_row - r_start_row) * n, MPI_INT, result[r_start_row].data(), (r_end_row - r_start_row) * n, MPI_INT, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Gather(local_result.data(), local_result.size() * n, MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Finalize();\n}",
        "framework": "MPI"
      }
    },
    {
      "metadata": {
        "task_type": "graph_bfs",
        "hardware": {
          "cpu": {
            "cores": 8,
            "threads": 16,
            "frequency": "3.6 GHz",
            "available": "True"
          },
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <mpi.h>\n#include <vector>\n#include <queue>\n#include <iostream>\n\nvoid bfs(const Graph& graph, int start) {\n    int rank, size;\n    MPI_Init(NULL, NULL);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = graph.size();\n    std::vector<bool> visited(n, false);\n    std::queue<int> q;\n\n    if (rank == 0) {\n        visited[start] = true;\n        q.push(start);\n    }\n\n    while (true) {\n        std::vector<int> local_queue;\n        while (!q.empty()) {\n            int node = q.front();\n            q.pop();\n            local_queue.push_back(node);\n        }\n\n        int local_size = local_queue.size();\n        std::vector<int> global_sizes(size);\n        MPI_Allgather(&local_size, 1, MPI_INT, global_sizes.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n        int total_size = 0;\n        for (int s : global_sizes) {\n            total_size += s;\n        }\n\n        std::vector<int> all_nodes(total_size);\n        int offset = 0;\n        for (int i = 0; i < rank; ++i) {\n            offset += global_sizes[i];\n        }\n        MPI_Allgatherv(local_queue.data(), local_size, MPI_INT, all_nodes.data(), global_sizes.data(), offsets.data(), MPI_INT, MPI_COMM_WORLD);\n\n        std::vector<int> next_level;\n        for (int node : all_nodes) {\n            for (int neighbor : graph[node]) {\n                if (!visited[neighbor]) {\n                    visited[neighbor] = true;\n                    next_level.push_back(neighbor);\n                }\n            }\n        }\n\n        local_size = next_level.size();\n        MPI_Allgather(&local_size, 1, MPI_INT, global_sizes.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n        total_size = 0;\n        for (int s : global_sizes) {\n            total_size += s;\n        }\n\n        all_nodes.resize(total_size);\n        offset = 0;\n        for (int i = 0; i < rank; ++i) {\n            offset += global_sizes[i];\n        }\n        MPI_Allgatherv(next_level.data(), local_size, MPI_INT, all_nodes.data(), global_sizes.data(), offsets.data(), MPI_INT, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            for (int node : all_nodes) {\n                std::cout << node << \" \";\n            }\n        }\n\n        bool done = all_nodes.empty();\n        MPI_Allreduce(MPI_IN_PLACE, &done, 1, MPI_CXX_BOOL, MPI_LAND, MPI_COMM_WORLD);\n\n        if (done) {\n            break;\n        }\n\n        for (int node : all_nodes) {\n            q.push(node);\n        }\n    }\n\n    MPI_Finalize();\n}",
        "framework": "MPI"
      }
    }
  ]
}