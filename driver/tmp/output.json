{
  "tasks": [
    {
      "metadata": {
        "task_type": "array_sum",
        "hardware": {
          "cpu": {
            "cores": 8,
            "threads": 16,
            "frequency": "3.6 GHz",
            "available": "True"
          },
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <vector>\n#include <cuda_runtime.h>\n\n// 定义数组类型\nusing Array = std::vector<int>;\n\n__global__ void sum_kernel(int* arr, int n, int* result) {\n    extern __shared__ int sdata[];\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    sdata[tid] = (i < n) ? arr[i] : 0;\n    __syncthreads();\n\n    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        atomicAdd(result, sdata[0]);\n    }\n}\n\nint array_sum(const Array& arr) {\n    int n = arr.size();\n    int* d_arr;\n    int* d_result;\n    int h_result = 0;\n\n    cudaMalloc(&d_arr, n * sizeof(int));\n    cudaMemcpy(d_arr, arr.data(), n * sizeof(int), cudaMemcpyHostToDevice);\n    cudaMalloc(&d_result, sizeof(int));\n    cudaMemset(d_result, 0, sizeof(int));\n\n    int blockSize = 256;\n    int numBlocks = (n + blockSize - 1) / blockSize;\n\n    sum_kernel<<<numBlocks, blockSize, blockSize * sizeof(int)>>>(d_arr, n, d_result);\n    cudaMemcpy(&h_result, d_result, sizeof(int), cudaMemcpyDeviceToHost);\n\n    cudaFree(d_arr);\n    cudaFree(d_result);\n\n    return h_result;\n}",
        "framework": "CUDA"
      }
    },
    {
      "metadata": {
        "task_type": "matrix_multiply",
        "hardware": {
          "cpu": {
            "cores": 8,
            "threads": 16,
            "frequency": "3.6 GHz",
            "available": "True"
          },
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <iostream>\n#include <vector>\n#include <cuda_runtime.h>\n\nusing Matrix = std::vector<int>;\n\n__global__ void matrixMultiplyKernel(int* A, int* result, int N, int M) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        int sum = 0;\n        for (int k = 0; k < M; ++k) {\n            sum += A[row * M + k] * A[col * M + k];\n        }\n        result[row * N + col] = sum;\n    }\n}\n\nvoid matrix_multiply(const Matrix& A, int N, int M, Matrix& result) {\n    int* d_A;\n    int* d_result;\n\n    size_t size_A = N * M * sizeof(int);\n    size_t size_result = N * N * sizeof(int);\n\n    cudaMalloc(&d_A, size_A);\n    cudaMalloc(&d_result, size_result);\n\n    cudaMemcpy(d_A, A.data(), size_A, cudaMemcpyHostToDevice);\n\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x, (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n\n    matrixMultiplyKernel<<<numBlocks, threadsPerBlock>>>(d_A, d_result, N, M);\n\n    cudaMemcpy(result.data(), d_result, size_result, cudaMemcpyDeviceToHost);\n\n    cudaFree(d_A);\n    cudaFree(d_result);\n}",
        "framework": "CUDA"
      }
    }
  ]
}