{
  "tasks": [
    {
      "metadata": {
        "task_type": "array_sum",
        "hardware": {
          "cpus": [
            {
              "cores": 12,
              "threads": 32,
              "frequency": "3.6 GHz",
              "available": "True"
            }
          ],
          "gpu": {
            "cuda_cores": 10496,
            "memory": {
              "size": "24 GB",
              "type": "GDDR6X"
            },
            "available": "True"
          }
        },
        "code": "#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n\n__global__ void sum_kernel(const long long* data, long long* partial_sums, int n) {\n    extern __shared__ long long sdata[];\n    \n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load data into shared memory\n    sdata[tid] = (i < n) ? data[i] : 0;\n    __syncthreads();\n    \n    // Perform reduction in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    // Write result for this block to global memory\n    if (tid == 0) {\n        partial_sums[blockIdx.x] = sdata[0];\n    }\n}\n\nlong long array_sum(const Array& arr) {\n    if (arr.empty()) return 0;\n    \n    int n = arr.size();\n    const int block_size = 512;\n    int num_blocks = (n + block_size - 1) / block_size;\n    \n    // Allocate device memory\n    long long* d_data;\n    long long* d_partial_sums;\n    cudaMalloc(&d_data, n * sizeof(long long));\n    cudaMalloc(&d_partial_sums, num_blocks * sizeof(long long));\n    \n    // Copy data to device\n    cudaMemcpy(d_data, arr.data(), n * sizeof(long long), cudaMemcpyHostToDevice);\n    \n    // Launch kernel\n    sum_kernel<<<num_blocks, block_size, block_size * sizeof(long long)>>>(d_data, d_partial_sums, n);\n    \n    // Copy partial sums back to host\n    std::vector<long long> partial_sums(num_blocks);\n    cudaMemcpy(partial_sums.data(), d_partial_sums, num_blocks * sizeof(long long), cudaMemcpyDeviceToHost);\n    \n    // Final reduction on CPU\n    long long total_sum = 0;\n    for (int i = 0; i < num_blocks; i++) {\n        total_sum += partial_sums[i];\n    }\n    \n    // Clean up\n    cudaFree(d_data);\n    cudaFree(d_partial_sums);\n    \n    return total_sum;\n}",
        "framework": "CUDA"
      }
    },
    {
      "metadata": {
        "task_type": "matrix_multiply",
        "hardware": {
          "cpus": [
            {
              "cores": 12,
              "threads": 32,
              "frequency": "3.6 GHz",
              "available": "True"
            }
          ],
          "gpu": {
            "cuda_cores": 10496,
            "memory": {
              "size": "24 GB",
              "type": "GDDR6X"
            },
            "available": "True"
          }
        },
        "code": "__global__ void matrix_multiply_kernel(const int* A, int* result, int N, int M) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (row < N && col < N) {\n        int sum = 0;\n        for (int k = 0; k < M; k++) {\n            sum += A[row * M + k] * A[col * M + k];\n        }\n        result[row * N + col] = sum;\n    }\n}\n\nvoid matrix_multiply(const Matrix& A, int N, int M, Matrix& result) {\n    result.resize(N * N);\n    \n    int* d_A;\n    int* d_result;\n    \n    size_t size_A = N * M * sizeof(int);\n    size_t size_result = N * N * sizeof(int);\n    \n    cudaMalloc(&d_A, size_A);\n    cudaMalloc(&d_result, size_result);\n    \n    cudaMemcpy(d_A, A.data(), size_A, cudaMemcpyHostToDevice);\n    \n    dim3 blockSize(16, 16);\n    dim3 gridSize((N + blockSize.x - 1) / blockSize.x, (N + blockSize.y - 1) / blockSize.y);\n    \n    matrix_multiply_kernel<<<gridSize, blockSize>>>(d_A, d_result, N, M);\n    \n    cudaMemcpy(result.data(), d_result, size_result, cudaMemcpyDeviceToHost);\n    \n    cudaFree(d_A);\n    cudaFree(d_result);\n}",
        "framework": "CUDA"
      }
    },
    {
      "metadata": {
        "task_type": "graph_bfs",
        "hardware": {
          "cpus": [
            {
              "cores": 12,
              "threads": 32,
              "frequency": "3.6 GHz",
              "available": "True"
            }
          ],
          "gpu": {
            "cuda_cores": 10496,
            "memory": {
              "size": "24 GB",
              "type": "GDDR6X"
            },
            "available": "True"
          }
        },
        "code": "__global__ void bfs_kernel(const int* offset, const int* edges, int* distances, bool* visited, bool* frontier, bool* next_frontier, int numVertices, bool* has_work) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (tid < numVertices && frontier[tid]) {\n        int start = offset[tid];\n        int end = (tid + 1 < numVertices) ? offset[tid + 1] : 0;\n        if (tid + 1 >= numVertices) {\n            // Handle last vertex\n            end = start;\n            for (int i = 0; i < numVertices; i++) {\n                if (offset[i] > start) {\n                    end = offset[i];\n                    break;\n                }\n            }\n        }\n        \n        for (int i = start; i < end; i++) {\n            int neighbor = edges[i];\n            if (!visited[neighbor]) {\n                if (atomicCAS((int*)&visited[neighbor], 0, 1) == 0) {\n                    distances[neighbor] = distances[tid] + 1;\n                    next_frontier[neighbor] = true;\n                    *has_work = true;\n                }\n            }\n        }\n    }\n}\n\nvoid bfs(const CUDAGraph& graph, int start, std::vector<int>& result) {\n    int numVertices = graph.numVertices;\n    \n    // Device memory allocation\n    int* d_offset;\n    int* d_edges;\n    int* d_distances;\n    bool* d_visited;\n    bool* d_frontier;\n    bool* d_next_frontier;\n    bool* d_has_work;\n    \n    cudaMalloc(&d_offset, numVertices * sizeof(int));\n    cudaMalloc(&d_edges, graph.numEdges * sizeof(int));\n    cudaMalloc(&d_distances, numVertices * sizeof(int));\n    cudaMalloc(&d_visited, numVertices * sizeof(bool));\n    cudaMalloc(&d_frontier, numVertices * sizeof(bool));\n    cudaMalloc(&d_next_frontier, numVertices * sizeof(bool));\n    cudaMalloc(&d_has_work, sizeof(bool));\n    \n    // Copy graph data to device\n    cudaMemcpy(d_offset, graph.offset, numVertices * sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_edges, graph.edges, graph.numEdges * sizeof(int), cudaMemcpyHostToDevice);\n    \n    // Initialize arrays\n    cudaMemset(d_distances, -1, numVertices * sizeof(int));\n    cudaMemset(d_visited, false, numVertices * sizeof(bool));\n    cudaMemset(d_frontier, false, numVertices * sizeof(bool));\n    \n    // Set starting vertex\n    int start_distance = 0;\n    bool start_visited = true;\n    bool start_frontier = true;\n    cudaMemcpy(&d_distances[start], &start_distance, sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(&d_visited[start], &start_visited, sizeof(bool), cudaMemcpyHostToDevice);\n    cudaMemcpy(&d_frontier[start], &start_frontier, sizeof(bool), cudaMemcpyHostToDevice);\n    \n    // Calculate grid and block dimensions\n    int blockSize = 256;\n    int gridSize = (numVertices + blockSize - 1) / blockSize;\n    \n    bool has_work = true;\n    while (has_work) {\n        has_work = false;\n        cudaMemcpy(d_has_work, &has_work, sizeof(bool), cudaMemcpyHostToDevice);\n        cudaMemset(d_next_frontier, false, numVertices * sizeof(bool));\n        \n        bfs_kernel<<<gridSize, blockSize>>>(d_offset, d_edges, d_distances, d_visited, d_frontier, d_next_frontier, numVertices, d_has_work);\n        cudaDeviceSynchronize();\n        \n        cudaMemcpy(&has_work, d_has_work, sizeof(bool), cudaMemcpyDeviceToHost);\n        \n        // Swap frontiers\n        bool* temp = d_frontier;\n        d_frontier = d_next_frontier;\n        d_next_frontier = temp;\n    }\n    \n    // Copy results back to host\n    result.resize(numVertices);\n    cudaMemcpy(result.data(), d_distances, numVertices * sizeof(int), cudaMemcpyDeviceToHost);\n    \n    // Count valid results and print size\n    int valid_count = 0;\n    for (int i = 0; i < numVertices; i++) {\n        if (result[i] != -1) {\n            valid_count++;\n        }\n    }\n    printf(\"%d\\n\", valid_count);\n    \n    // Free device memory\n    cudaFree(d_offset);\n    cudaFree(d_edges);\n    cudaFree(d_distances);\n    cudaFree(d_visited);\n    cudaFree(d_frontier);\n    cudaFree(d_next_frontier);\n    cudaFree(d_has_work);\n}",
        "framework": "CUDA"
      }
    }
  ]
}