{
  "tasks": [
    {
      "metadata": {
        "task_type": "array_sum",
        "hardware": {
          "cpus": [
            {
              "cores": 8,
              "threads": 16,
              "frequency": "3.6 GHz",
              "available": "True"
            },
            {
              "cores": 4,
              "threads": 8,
              "frequency": "3.2 GHz",
              "available": "True"
            }
          ],
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <vector>\n#include <cuda_runtime.h>\n\nusing Array = std::vector<int>;\n\n__global__ void sum_kernel(int* arr, int n, int* result) {\n    extern __shared__ int sdata[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    sdata[tid] = (i < n) ? arr[i] : 0;\n    __syncthreads();\n\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        atomicAdd(result, sdata[0]);\n    }\n}\n\nint array_sum(const Array& arr) {\n    int n = arr.size();\n    int* d_arr;\n    int* d_result;\n    int h_result = 0;\n\n    cudaMalloc(&d_arr, n * sizeof(int));\n    cudaMalloc(&d_result, sizeof(int));\n    cudaMemcpy(d_arr, arr.data(), n * sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemset(d_result, 0, sizeof(int));\n\n    int blockSize = 256;\n    int numBlocks = (n + blockSize - 1) / blockSize;\n    sum_kernel<<<numBlocks, blockSize, blockSize * sizeof(int)>>>(d_arr, n, d_result);\n\n    cudaMemcpy(&h_result, d_result, sizeof(int), cudaMemcpyDeviceToHost);\n\n    cudaFree(d_arr);\n    cudaFree(d_result);\n\n    return h_result;\n}",
        "framework": "CUDA"
      }
    },
    {
      "metadata": {
        "task_type": "matrix_multiply",
        "hardware": {
          "cpus": [
            {
              "cores": 8,
              "threads": 16,
              "frequency": "3.6 GHz",
              "available": "True"
            },
            {
              "cores": 4,
              "threads": 8,
              "frequency": "3.2 GHz",
              "available": "True"
            }
          ],
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <iostream>\n#include <vector>\n#include <cuda_runtime.h>\n\nusing Matrix = std::vector<int>;\n\n__global__ void matrixMultiplyKernel(const int* A, int N, int M, int* result) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int sum = 0;\n\n    if (row < N && col < M) {\n        for (int k = 0; k < M; ++k) {\n            sum += A[row * M + k] * A[col * M + k];\n        }\n        result[row * M + col] = sum;\n    }\n}\n\nvoid matrix_multiply(const Matrix& A, int N, int M, Matrix& result) {\n    int* d_A;\n    int* d_result;\n\n    size_t size = N * M * sizeof(int);\n\n    cudaMalloc(&d_A, size);\n    cudaMalloc(&d_result, size);\n\n    cudaMemcpy(d_A, A.data(), size, cudaMemcpyHostToDevice);\n\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks((M + threadsPerBlock.x - 1) / threadsPerBlock.x, (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n\n    matrixMultiplyKernel<<<numBlocks, threadsPerBlock>>>(d_A, N, M, d_result);\n\n    cudaMemcpy(result.data(), d_result, size, cudaMemcpyDeviceToHost);\n\n    cudaFree(d_A);\n    cudaFree(d_result);\n}",
        "framework": "CUDA"
      }
    },
    {
      "metadata": {
        "task_type": "graph_bfs",
        "hardware": {
          "cpus": [
            {
              "cores": 8,
              "threads": 16,
              "frequency": "3.6 GHz",
              "available": "True"
            },
            {
              "cores": 4,
              "threads": 8,
              "frequency": "3.2 GHz",
              "available": "True"
            }
          ],
          "gpu": {
            "cuda_cores": 3584,
            "memory": {
              "size": "12 GB",
              "type": "GDDR6"
            },
            "available": "True"
          }
        },
        "code": "#include <iostream>\n#include <cuda_runtime.h>\n\n__global__ void bfsKernel(int* visited, int* frontier, int* newFrontier, const int* offset, const int* edges, int numVertices, int* frontierSize) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < *frontierSize) {\n        int node = frontier[idx];\n        for (int i = offset[node]; i < offset[node + 1]; ++i) {\n            int neighbor = edges[i];\n            if (!visited[neighbor]) {\n                int pos = atomicExch(&visited[neighbor], 1);\n                if (!pos) {\n                    int index = atomicAdd(newFrontier + numVertices, 1);\n                    newFrontier[index] = neighbor;\n                }\n            }\n        }\n    }\n}\n\nvoid bfs(const CUDAGraph& graph, int start) {\n    int numVertices = graph.numVertices;\n    int* d_visited, *d_frontier, *d_newFrontier, *d_offset, *d_edges, *d_frontierSize;\n    int h_frontier[numVertices], h_newFrontier[numVertices], h_visited[numVertices] = {0};\n\n    h_visited[start] = 1;\n    h_frontier[0] = start;\n    int frontierSize = 1;\n\n    cudaMalloc(&d_visited, numVertices * sizeof(int));\n    cudaMalloc(&d_frontier, numVertices * sizeof(int));\n    cudaMalloc(&d_newFrontier, numVertices * sizeof(int));\n    cudaMalloc(&d_offset, (numVertices + 1) * sizeof(int));\n    cudaMalloc(&d_edges, graph.numEdges * sizeof(int));\n    cudaMalloc(&d_frontierSize, sizeof(int));\n\n    cudaMemcpy(d_visited, h_visited, numVertices * sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_frontier, h_frontier, numVertices * sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_offset, graph.offset, (numVertices + 1) * sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_edges, graph.edges, graph.numEdges * sizeof(int), cudaMemcpyHostToDevice);\n\n    dim3 blockSize(256);\n    dim3 gridSize((numVertices + blockSize.x - 1) / blockSize.x);\n\n    while (frontierSize > 0) {\n        cudaMemcpy(d_frontierSize, &frontierSize, sizeof(int), cudaMemcpyHostToDevice);\n        cudaMemset(d_newFrontier + numVertices, 0, sizeof(int));\n        bfsKernel<<<gridSize, blockSize>>>(d_visited, d_frontier, d_newFrontier, d_offset, d_edges, numVertices, d_frontierSize);\n        cudaMemcpy(h_frontier, d_frontier, numVertices * sizeof(int), cudaMemcpyDeviceToHost);\n        cudaMemcpy(h_newFrontier, d_newFrontier, numVertices * sizeof(int), cudaMemcpyDeviceToHost);\n        cudaMemcpy(&frontierSize, d_frontierSize, sizeof(int), cudaMemcpyDeviceToHost);\n\n        for (int i = 0; i < frontierSize; ++i) {\n            std::cout << h_frontier[i] << \" \";\n        }\n\n        std::swap(h_frontier, h_newFrontier);\n    }\n\n    cudaFree(d_visited);\n    cudaFree(d_frontier);\n    cudaFree(d_newFrontier);\n    cudaFree(d_offset);\n    cudaFree(d_edges);\n    cudaFree(d_frontierSize);\n}",
        "framework": "CUDA"
      }
    }
  ]
}